{"paragraphs":[{"text":"%spark.pyspark\nfrom pyspark.sql import SQLContext\nimport os\nimport re\nimport sys\nimport unicodedata\nimport itertools\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAJ5WLA5RSQOM7VFVQ\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n\nsqlContext = SQLContext(sc)\ndfParquet = sqlContext.read.parquet(\"s3a://msdbucket/parquet\")","user":"anonymous","dateUpdated":"2020-04-18T23:41:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1587240403817_747787038","id":"20200418-094817_231359692","dateCreated":"2020-04-18T20:06:43+0000","dateStarted":"2020-04-18T23:41:31+0000","dateFinished":"2020-04-18T23:41:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2253"},{"text":"%spark.pyspark\n# ROTATION SYMBOLS (A and B => B and A)\nrotation_symbols = ['\\|', '/', '&', ',', '\\+', ';', '_']#, '\\-']\nrotation_words = ['and', 'y', 'et', 'vs', 'vs.', 'v', 'with', 'feat',\n                  'feat.', 'featuring', 'presents', 'ft.', 'pres.']\n\n# SYMBOLS TO REMOVE AT THE BEGINNING\nstub_to_remove = ['dj', 'dj.', 'mc', 'm.c.', 'mc.', 'the', 'los', 'les']\n\n# SYMBOLS TO REMOVE AT THE END\nend_to_remove1 = ['big band', 'trio', 'quartet', 'ensemble', 'orchestra']\nend_to_remove2 = ['band']\n\n# COMPILED REGULAR EXPRESSION\n# white spaces\nre_space = re.compile(r'\\s')\n# non alphanumeric\nre_nonalphanum = re.compile(r'\\W')\n# rotation symbols\nre_rotsymbols = re.compile('\\s*?' + '|'.join(rotation_symbols) + '\\s*?')\n# rotation words\nre_rotwords = re.compile(r'\\s(' + '|'.join(rotation_words) + ')\\s')\n# stub to remove\nre_remstub = re.compile('(' + '|'.join(stub_to_remove) + ')\\s(.*)')\n# ending to remove\nre_remending1 = re.compile('(.*)\\s(' + '|'.join(end_to_remove1) + ')')\nre_remending2 = re.compile('(.*)\\s(' + '|'.join(end_to_remove2) + ')')\n# quotes to remove\nre_remquotes = re.compile('(.+)\\s(\".+?\")\\s(.+)')\n# parenthesis to remove\nre_remparenthesis = re.compile('(.+)\\s(\\(.+?\\))\\s*(.*)')\n# brackets to remove\nre_rembrackets = re.compile('(.+)\\s(\\[.+?\\])\\s*(.*)')\n\n\ndef char_is_ascii(c):\n    \"\"\"\n    Check if a unicode character, e.g. u'A', u'1' or u'\\u0301' is ASCII\n    \"\"\"\n    #return ord(c) < 128\n    # the following should be faster, according to:\n    #http://stackoverflow.com/questions/196345/how-to-check-if-a-string-in-python-is-in-ascii\n    return c < u\"\\x7F\"\n\n\ndef remove_non_ascii(s):\n    \"\"\"\n    Normalize characters in unicode string 's' that are not ASCII,\n    try to transform accented characters to non accented version.\n    Otherwise, remove non-ascii chars\n    \"\"\"\n    decomposition = unicodedata.normalize('NFKD', s)\n    return \"\".join(list(filter(lambda x: char_is_ascii(x), decomposition)))\n\n\ndef to_lower_case(s):\n    \"\"\"\n    transform a unicode string 's' to lowercase\n    ok, this one is trivial, I know\n    \"\"\"\n    return s.lower()\n\n\ndef remove_spaces(s):\n    \"\"\"\n    Remove all possible spaces in the unicode string s\n    \"\"\"\n    return re_space.sub('', s)\n\n\ndef replace_rotation_symbols(s):\n    \"\"\"\n    Mostly, replace '&' by 'and'\n    \"\"\"\n    return re_rotsymbols.sub(' and ', s)\n\n\ndef remove_stub(s):\n    \"\"\"\n    Remove a questionable beginning, e.g. dj\n    otherwise return string at is\n    \"\"\"\n    m = re_remstub.match(s)\n    if not m:\n        return s\n    return m.groups()[1]\n\n\ndef remove_endings(s):\n    \"\"\"\n    Remove questionable endings, e.g. 'band'\n    \"\"\"\n    m = re_remending1.match(s)\n    if m:\n       s = m.groups()[0]\n    m = re_remending2.match(s)\n    if m:\n        s = m.groups()[0]\n    return s\n\n\ndef remove_quotes(s):\n    \"\"\"\n    Remove the quote, like Thierry \"The Awesomest\" BM\n    \"\"\"\n    m = re_remquotes.match(s)\n    if not m:\n        return s\n    parts = m.groups()\n    assert len(parts) == 3\n    return parts[0] + ' ' + parts[2]\n\n\ndef remove_parenthesis(s):\n    \"\"\"\n    Remove parenthesis, like Thierry (Coolest guy)\n    \"\"\"\n    print(s)\n    m = re_remparenthesis.match(s)\n    if not m:\n        return s\n    parts = m.groups()\n    assert len(parts) >= 2\n    if len(parts) == 2:\n        return parts[0]\n    return parts[0] + ' ' + parts[2]\n\n\ndef remove_brackets(s):\n    \"\"\"\n    Remove brackets, like Thierry [Coolest guy]\n    \"\"\"\n    m = re_rembrackets.match(s)\n    if not m:\n        return s\n    parts = m.groups()\n    assert len(parts) >= 2\n    if len(parts) == 2:\n        return parts[0]\n    return parts[0] + ' ' + parts[2]\n\n\ndef normalize_no_rotation(s):\n    \"\"\"\n    We normalize a name that is supposed to contain no\n    rotation term ('and', 'y', ...)\n    \"\"\"\n    # remove beginning\n    s = remove_stub(s)\n    # remove ends\n    s = remove_endings(s)    \n    # remove ()\n    s = remove_parenthesis(s)\n    # remove \"\"\n    s = remove_quotes(s)\n    return s\n\n\ndef split_rotation_words(s):\n    \"\"\"\n    Split a name using the rotation words: 'and', 'vs', 'y', 'et', ...\n    then create all possible permutations\n    \"\"\"\n    parts = re_rotwords.split(s)\n    parts = list(filter(lambda p: not p in rotation_words, parts))[:5]\n    results = set()\n    # keep only the individual elems (risky?)\n    for p in parts:\n        results.add(p)\n    # create all permutations\n    permutations = itertools.permutations(parts)\n    #maxperm = 30\n    #count_perm = 0\n    for perm in permutations:\n        #count_perm += 1\n        #if count_perm > maxperm:\n        #    break\n        results.add(' '.join(perm))\n    # redo the same but remove the stub first for all parts\n    parts = list(map(lambda p: normalize_no_rotation(p), parts))\n    for p in parts:\n        results.add(p)\n    permutations = itertools.permutations(parts)\n    for perm in permutations:\n        results.add(' '.join(perm))\n    # done\n    return results\n\n\ndef remove_nonalphanumeric(s):\n    \"\"\"\n    Remove usual punctuation signs:  ! , ? : ; . '   etc\n    Also, we transform long spaces into normal ones\n    \"\"\"\n    # split around non-alphanum chars\n    parts = re_nonalphanum.split(s)\n    # remove empty spots\n    parts = list(filter(lambda p: p, parts))\n    # rejoin with regular space ' '\n    return ' '.join(parts)\n\n\ndef normalize_artist(s):\n    \"\"\"\n    Return a set of normalized versions of that artist name\n    \"\"\"\n    # normalized versions\n    results = set()\n    # lower case\n    s = to_lower_case(s)\n    results.add(s)\n    print(s)\n    # remove non-ascii chars (try to replace them)\n    s = remove_non_ascii(s)\n    print(s)\n    results.add(s)\n    # try removing parenthesis before, in case there's an & in it\n    s2 = remove_parenthesis(s)\n    results.add(s2)\n    # replace rotation symbols\n    s = replace_rotation_symbols(s)\n    # split and permute according to rotation words\n    permutations = split_rotation_words(s)\n    results.update(permutations)\n    print(s)\n    # remove non-alphanumeric and normalize spaces\n    results = list(map(lambda s: remove_nonalphanumeric(s), results))\n    # remove all spaces\n    results = list(map(lambda s: remove_spaces(s), results))\n    # done (and remove dupes)\n    return set(results)\n\n\ndef normalize_title(s):\n    \"\"\"\n    Return a set of normalized versions of that title\n    \"\"\"\n    # normalized versions\n    results = set()\n    # lower case\n    s = to_lower_case(s)\n    results.add(s)\n    # remove non-ascii chars (try to replace them)\n    s = remove_non_ascii(s)\n    results.add(s)\n    # try removing parenthesis\n    s = remove_parenthesis(s)\n    results.add(s)\n    # try removing brackets\n    s = remove_brackets(s)\n    results.add(s)\n    # remove non-alphanumeric and normalize spaces\n    results = map(lambda s: remove_nonalphanumeric(s), results)\n    # remove all spaces\n    results = map(lambda s: remove_spaces(s), results)\n    # done (and remove dupes)\n    return set(results)","user":"anonymous","dateUpdated":"2020-04-18T23:41:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1587240403818_-1852460131","id":"20200418-100325_570748945","dateCreated":"2020-04-18T20:06:43+0000","dateStarted":"2020-04-18T23:41:39+0000","dateFinished":"2020-04-18T23:41:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2254"},{"text":"%spark.pyspark\nimport pyspark.sql.functions as F\nimport pyspark.sql.types as T\n\ndef normalize_name_udf(col):\n    return sorted(list(normalize_artist(col)), key=len)[0]\n\ndef normalize_title_udf(col):\n    return sorted(list(normalize_title(col)), key=len)[0]\n    \ndef normalize(col):\n    return col.lower()\n\ndef group_normalize(col):\n    return [c.lower() for c in col]\n\n# if we assume that my_func returns a string\nudf_name = F.UserDefinedFunction(normalize_name_udf, T.StringType())\nudf_title = F.UserDefinedFunction(normalize_title_udf, T.StringType())\nudf_lower = F.UserDefinedFunction(normalize, T.StringType())\nudf_group_lower = F.UserDefinedFunction(group_normalize)\n\ndfParquet = dfParquet.withColumn('artist_name', udf_name('artist_name'))\ndfParquet = dfParquet.withColumn('title', udf_title('title'))\ndfParquet = dfParquet.withColumn('release', udf_title('release'))\ndfParquet = dfParquet.withColumn('artist_id', udf_lower('artist_id'))\ndfParquet = dfParquet.withColumn('song_id', udf_lower('song_id'))\ndfParquet = dfParquet.withColumn('track_id', udf_lower('track_id'))\ndfParquet = dfParquet.withColumn('artist_location', udf_lower('artist_location'))\ndfParquet = dfParquet.withColumn('similar_artists', udf_group_lower('similar_artists'))","user":"anonymous","dateUpdated":"2020-04-18T23:41:42+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1587240403818_-2066859194","id":"20200418-094832_284911326","dateCreated":"2020-04-18T20:06:43+0000","dateStarted":"2020-04-18T23:41:42+0000","dateFinished":"2020-04-18T23:41:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2255"},{"text":"%spark.pyspark\ndfParquet.select('title').show()","user":"anonymous","dateUpdated":"2020-04-18T23:41:45+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+\n|               title|\n+--------------------+\n|             journey|\n|        tentimesblue|\n|           itssogood|\n|              ghosts|\n|  babypleasecomehome|\n|akdotedavoddelalt...|\n|         bonjourcava|\n|overthehillsandfa...|\n|   endangeredspecies|\n|         gottagetyou|\n| toomuchofagoodthing|\n|            thebrawl|\n|   etsaiabesteakdira|\n|            tigerrag|\n|          iwontdance|\n|          wellbeokay|\n|               dream|\n|          osarracino|\n|  kostelbiohazardmcd|\n|          thesunroad|\n+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1587240403819_26575243","id":"20200418-095721_1424894473","dateCreated":"2020-04-18T20:06:43+0000","dateStarted":"2020-04-18T23:41:45+0000","dateFinished":"2020-04-18T23:41:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2256"},{"text":"%spark.pyspark\nfrom pyspark.ml.feature import FeatureHasher\nfrom pyspark.sql.types import *\n\ndataset = dfParquet.select(['artist_name', 'title', 'release', 'artist_id', 'song_id', 'track_id', 'artist_location']) \nhasher = FeatureHasher(inputCols=['artist_name', 'title', 'release', 'artist_id', 'song_id', 'track_id', 'artist_location'],\n                       outputCol=\"features\")\nfeaturized = hasher.transform(dataset)\nvector_udf = F.UserDefinedFunction(lambda vector: vector.toArray().tolist(),ArrayType(DoubleType()))\ncolvalues = featurized.select(vector_udf('features').alias('features')).show()","user":"anonymous","dateUpdated":"2020-04-18T23:43:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+\n|            features|\n+--------------------+\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n|[0.0, 0.0, 0.0, 0...|\n+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1587242548506_2024566932","id":"20200418-204228_903129046","dateCreated":"2020-04-18T20:42:28+0000","dateStarted":"2020-04-18T23:43:37+0000","dateFinished":"2020-04-18T23:44:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2257"},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2020-04-18T21:35:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1587245723742_-241874668","id":"20200418-213523_1826916850","dateCreated":"2020-04-18T21:35:23+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2258"}],"name":"Test","id":"2F7976CHK","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}
